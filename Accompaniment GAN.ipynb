{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Formule output width of convolution network (same for height)\n",
    "# W=(W−K+2P)/S+1 where W=width, K=kernel width, P=padding and S=stride\n",
    "\n",
    "# Formule output width of transpose convolution network (same for height)\n",
    "# W=(W−1)*S+K-2P where W=width, K=kernel width, P=padding and S=stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN for musical accompaniement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Code in .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(folder=npy_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This module extends the given noise in the time dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim, n_bars):\n",
    "        super(TemporalNetwork, self).__init__()\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, 1024, (2, 1), stride=(1, 1)),\n",
    "            nn.BatchNorm2d(1024, momentum=0.9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, z_dim, (n_bars - 1, 1), stride=(1, 1)),\n",
    "            nn.BatchNorm2d(z_dim, momentum=0.9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        nn.init.kaiming_normal_(self.layer_1[0].weight)\n",
    "        nn.init.kaiming_normal_(self.layer_2[0].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input if of size (batch, 1 bar, z_dim, 1)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        # Input if of size (batch, z_dim, 1, 1)\n",
    "        x = self.layer_1(x)\n",
    "        # Input if of size (batch, 1024, 2, 1)\n",
    "        x = self.layer_2(x)\n",
    "        # Input if of size (batch, z_dim, n_bars, 1)\n",
    "        x.permute(0, 2, 1, 3)\n",
    "        # Input if of size (batch, n_bars, z_dim, 1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This module encodes a whole bar over all tracks as a 1-dim embedding to create short-term memory for\n",
    "    the Bar Generator\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,output_dim):\n",
    "        super(BarEncoder, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(5, 16, (3,12), stride=(3,4), padding=(0,0)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d((2,2))\n",
    "        )\n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, (2,3), stride=(2,2), padding=(0,0)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d((2,1))\n",
    "        )\n",
    "\n",
    "        nn.init.kaiming_normal_(self.layer_1[0].weight)\n",
    "        nn.init.kaiming_normal_(self.layer_2[0].weight)\n",
    "\n",
    "        self.linear = nn.Linear(in_features=448, out_features=output_dim)\n",
    "\n",
    "    def forward(self, bar):\n",
    "        # The bar is of shape (batch,5,1,96,128) = (batch,n_tracks,n_bars,n_steps_per_bat,n_pitches)\n",
    "        bar = bar.squeeze(2) \n",
    "        # Now the shape is (batch,5,96,128)\n",
    "        out = self.layer_1(bar)\n",
    "        # Now the shape is (batch,16,16,15)\n",
    "        out = self.layer_2(out)\n",
    "        # Now the shape is (batch,16,4,7)\n",
    "        out = nn.Flatten()(out)\n",
    "        # Now the shape is (batch,448)\n",
    "        out = self.linear(out)\n",
    "        # Now the shape is (batch, output_shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelodyEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This module encodes the conditionnal melody given as input as a 1-dim embedding to feed\n",
    "    to the Bar Generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim):\n",
    "        super(MelodyEncoder, self).__init__()\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.Conv3d(1, 32, (1, 4, 12), stride=(1, 4, 4)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((1, 2, 2)),\n",
    "        )\n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, (1, 3, 3), stride=(1, 3, 2), padding=(0,0,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((1, 2, 2)),\n",
    "        )\n",
    "        self.layer_3 = nn.Linear(512, output_dim)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.layer_1[0].weight)\n",
    "        nn.init.kaiming_normal_(self.layer_2[0].weight)\n",
    "\n",
    "    def forward(self, melody):\n",
    "        # Dimension of melody is (batch, 1, N_bars, 96, 128) = (batch,n_tracks,n_bars,n_steps_per_bat,n_pitches)\n",
    "        x = self.layer_1(melody)\n",
    "        # Output is of dimension (batch, 32, N_bars, 12, 15)\n",
    "        x = self.layer_2(x)\n",
    "        # Output is of dimension (batch, 64, N_bars, 2, 4)\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        # Output is of dimension (batch, N_bars, 64, 2, 4)\n",
    "        x = x.reshape(melody.shape[0], melody.shape[2], -1)\n",
    "        # Output is of dimension (batch, N_bars, 512)\n",
    "        x = self.layer_3(x)\n",
    "        # Output is of dimension (batch, N_bars, output_dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    This module uses 1-dim input, extends it along both the time and the pitch axis and creates the\n",
    "    next bar for the given track.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(BarGenerator, self).__init__()\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            #nn.BatchNorm1d(1024, momentum=0.9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, (2, 1), stride=(2, 1)),\n",
    "            #nn.BatchNorm2d(512, momentum=0.9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer_3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, (2, 1), stride=(2, 1)),\n",
    "            #nn.BatchNorm2d(256, momentum=0.9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer_4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, (2, 1), stride=(2, 1)),\n",
    "            #nn.BatchNorm2d(256, momentum=0.9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer_5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, (2, 1), stride=(2, 1)),\n",
    "            #nn.BatchNorm2d(256, momentum=0.9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer_6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, (2, 1), stride=(2, 1)),\n",
    "            #nn.BatchNorm2d(256, momentum=0.9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer_7 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, (3, 1), stride=(3, 1)),\n",
    "            #nn.BatchNorm2d(256, momentum=0.9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer_8 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, (1, 4), stride=(1, 4)),\n",
    "            #nn.BatchNorm2d(256, momentum=0.9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer_9 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, (1, 4), stride=(1, 4)),\n",
    "            #nn.BatchNorm2d(128, momentum=0.9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer_10 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 1, (1, 12), stride=(1, 8), padding=(0, 2)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        nn.init.kaiming_normal_(self.layer_2[0].weight)\n",
    "        nn.init.kaiming_normal_(self.layer_3[0].weight)\n",
    "        nn.init.kaiming_normal_(self.layer_4[0].weight)\n",
    "        nn.init.kaiming_normal_(self.layer_5[0].weight)\n",
    "        nn.init.kaiming_normal_(self.layer_6[0].weight)\n",
    "        nn.init.kaiming_normal_(self.layer_7[0].weight)\n",
    "        nn.init.kaiming_normal_(self.layer_8[0].weight)\n",
    "        nn.init.kaiming_normal_(self.layer_9[0].weight)\n",
    "        nn.init.kaiming_normal_(self.layer_10[0].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Dimension of input in (batch, 1 bar, input_dim, 1)\n",
    "        x = x.squeeze(3).squeeze(1)\n",
    "        # Dimension of x is (batch, input_dim)\n",
    "        x = self.layer_1(x)\n",
    "        # Dimension of x is (batch, 1024)\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        # Dimension of x is (batch, 1024, 1, 1)\n",
    "        x = self.layer_2(x)\n",
    "        # Dimension of x is (batch, 512, 2, 1)\n",
    "        x = self.layer_3(x)\n",
    "        # Dimension of x is (batch, 256, 4, 1)\n",
    "        x = self.layer_4(x)\n",
    "        # Dimension of x is (batch, 256, 8, 1)\n",
    "        x = self.layer_5(x)\n",
    "        # Dimension of x is (batch, 256, 16, 1)\n",
    "        x = self.layer_6(x)\n",
    "        # Dimension of x is (batch, 256, 32, 1)\n",
    "        x = self.layer_7(x)\n",
    "        # Dimension of x is (batch, 256, 96, 1)\n",
    "        x = self.layer_8(x)\n",
    "        # Dimension of x is (batch, 256, 96, 4)\n",
    "        x = self.layer_9(x)\n",
    "        # Dimension of x is (batch, 128, 96, 16)\n",
    "        x = self.layer_10(x)\n",
    "        # Dimension of output is (batch, 1, 96, 128)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Main parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    The Generator of the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, output_shape, z_dim, melody_output_dim, bar_encoder_output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.n_tracks = output_shape[0]\n",
    "        self.n_bars = output_shape[1]\n",
    "        self.bar_encoder_output_dim = bar_encoder_output_dim\n",
    "\n",
    "        self.melody_encoder = MelodyEncoder(melody_output_dim)\n",
    "        self.chord_temporal_network = TemporalNetwork(z_dim, self.n_bars)\n",
    "        self.track_temporal_networks = [TemporalNetwork(z_dim, self.n_bars) for _ in range(self.n_tracks - 1)]\n",
    "        self.bar_encoder = BarEncoder(bar_encoder_output_dim)\n",
    "        self.track_bar_generators = [BarGenerator(melody_output_dim + z_dim * 4 + bar_encoder_output_dim) for _ in\n",
    "                                     range(self.n_tracks - 1)]\n",
    "\n",
    "        # Register parameters:\n",
    "        for i,tempnet in enumerate(self.track_temporal_networks):\n",
    "            for j,param in enumerate(tempnet.parameters()):\n",
    "                self.register_parameter(\"tempnet\"+str(i)+'_'+str(j),param)\n",
    "        for i,bargen in enumerate(self.track_bar_generators):\n",
    "            for j,param in enumerate(bargen.parameters()):\n",
    "                self.register_parameter('bargen'+str(i)+'_'+str(j),param)\n",
    "\n",
    "    def forward(self, chord_noise, style_noise, tracks_noise, groove_noise, conditionnal_melody):\n",
    "        \"\"\"\n",
    "          Chord noise: (batch, 1 bar, z_dim, 1)\n",
    "          Style noise: (batch, 1 bar, z_dim, 1)\n",
    "          Tracks noise: (batch, N-1 tracks, 1 bar, z_dim, 1)\n",
    "          Groove noise: (batch, N-1 tracks, 1 bar, z_dim, 1)\n",
    "          Conditional melody: (batch, 1 track, N bars, 96, 128)\n",
    "        \"\"\"\n",
    "        batch_size = chord_noise.shape[0]\n",
    "\n",
    "        ####################### Build inputs #######################\n",
    "        # Encoded melody is (batch, N_bars, melody_output_dim, 1)\n",
    "        encoded_melody = self.melody_encoder(conditionnal_melody).unsqueeze(-1)\n",
    "\n",
    "        # Chord noise extended is (batch, N bars, z_dim, 1)\n",
    "        chord_extended = self.chord_temporal_network(chord_noise)\n",
    "\n",
    "        # Tracks noise extended is (N tracks, (batch, N bars, z_dim, 1))\n",
    "        tracks_noise_extended = []\n",
    "        for track in range(self.n_tracks - 1):\n",
    "            track_noise_extended = self.track_temporal_networks[track](tracks_noise[:,track].unsqueeze(1))\n",
    "            tracks_noise_extended.append(track_noise_extended)\n",
    "\n",
    "        ####################### Concatenate #######################\n",
    "        # Track bars will be dimension (N_bars, N_tracks-1, (batch, 1, melody_output_dim + z_dim * 4, 1))\n",
    "        track_bars = []\n",
    "\n",
    "        for bar in range(self.n_bars):\n",
    "            tracks = []\n",
    "            for track in range(self.n_tracks - 1):\n",
    "                tracks.append(torch.cat((\n",
    "                    style_noise.squeeze(1), #(batch, z_dim, 1)\n",
    "                    groove_noise[:, track, :, :, :].squeeze(1), #(batch, z_dim, 1)\n",
    "                    chord_extended[:, :, bar, :], #(batch, z_dim, 1)\n",
    "                    tracks_noise_extended[track][:, :, bar, :] #(batch, z_dim, 1)\n",
    "                ), dim=1).unsqueeze(1) #(batch ,1 bar, z_dim*4, 1)\n",
    "                             )\n",
    "                track_bars.append(tracks)\n",
    "\n",
    "        ####################### Generate bars #######################\n",
    "        previous_bar = torch.zeros(batch_size, 1, self.bar_encoder_output_dim, 1)\n",
    "        # Store whole bars\n",
    "        song = []\n",
    "        for bar in range(self.n_bars):\n",
    "            generated_tracks_bar = []\n",
    "            for track in range(self.n_tracks-1):\n",
    "                # Concat previous bar: (batch, 1 bar, melody_output_dim + z_dim * 4 + bar_encoder_output_dim, 1)\n",
    "                track_bar = torch.cat((encoded_melody[:,bar].unsqueeze(1),track_bars[bar][track], previous_bar), dim=2)\n",
    "                # Generated track bar: (batch, 1 bar, 96, 128)\n",
    "                generated_track_bar = self.track_bar_generators[track](track_bar)\n",
    "                # Append unsqueezed track bar: (batch, 1 track, 1 bar, 96, 128)\n",
    "                generated_tracks_bar.append(generated_track_bar.unsqueeze(1))\n",
    "            # Concat track: (batch, N-1 tracks, 1 bar, 96, 128)\n",
    "            whole_bar = torch.cat(generated_tracks_bar, dim=1)\n",
    "            # Add conditional melody: (batch, N tracks, 1 bar, 96, 128)\n",
    "            whole_song_bar = torch.cat((conditionnal_melody[:,:,bar].unsqueeze(2),whole_bar), dim=1)\n",
    "            # Unsqueeze encoded previous bar (batch, bar_encoder_output_dim) -> (batch, 1, bar_encoder_output_dim, 1)\n",
    "            previous_bar = self.bar_encoder(whole_song_bar).unsqueeze(1).unsqueeze(-1)\n",
    "            # Store whole bar\n",
    "            song.append(whole_song_bar)\n",
    "\n",
    "        # Concat bars: (batch, N tracks, 1 bar, 96, 128) -> (batch, N tracks, N bars, 96, 128)\n",
    "        song = torch.cat(song, dim=2)\n",
    "\n",
    "        return song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    The Discriminator of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        n_bars = input_dim[1]\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=5,\n",
    "                      out_channels=128,\n",
    "                      kernel_size=(2,1,1),\n",
    "                      stride=(1,1,1),\n",
    "                      padding=0,\n",
    "                      ),\n",
    "            nn.LeakyReLU()\n",
    "            )\n",
    "        nn.init.kaiming_normal_(self.conv1[0].weight)\n",
    "        \n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=128,\n",
    "                      out_channels=128,\n",
    "                      kernel_size=(n_bars-1,1,1),\n",
    "                      stride=(1,1,1),\n",
    "                      padding=0,\n",
    "                      ),\n",
    "            nn.LeakyReLU()\n",
    "            )\n",
    "        nn.init.kaiming_normal_(self.conv2[0].weight)\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=128,\n",
    "                      out_channels=128,\n",
    "                      kernel_size=(1,1,12),\n",
    "                      stride=(1,1,12),\n",
    "                      padding=(0,0,2),\n",
    "                      ),\n",
    "            nn.LeakyReLU()\n",
    "            )\n",
    "        nn.init.kaiming_normal_(self.conv3[0].weight)\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=128,\n",
    "                      out_channels=128,\n",
    "                      kernel_size=(1,1,7),\n",
    "                      stride=(1,1,4),\n",
    "                      padding=0,\n",
    "                      ),\n",
    "            nn.LeakyReLU()\n",
    "            )\n",
    "        nn.init.kaiming_normal_(self.conv4[0].weight)\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=128,\n",
    "                      out_channels=128,\n",
    "                      kernel_size=(1,2,1),\n",
    "                      stride=(1,2,1),\n",
    "                      padding=(0,0,0),\n",
    "                      ),\n",
    "            nn.LeakyReLU()\n",
    "            )\n",
    "        nn.init.kaiming_normal_(self.conv5[0].weight)\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=128,\n",
    "                      out_channels=128,\n",
    "                      kernel_size=(1,2,1),\n",
    "                      stride=(1,2,1),\n",
    "                      padding=(0,0,0),\n",
    "                      ),\n",
    "            nn.LeakyReLU()\n",
    "            )\n",
    "        nn.init.kaiming_normal_(self.conv6[0].weight)\n",
    "\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=128,\n",
    "                      out_channels=256,\n",
    "                      kernel_size=(1,4,1),\n",
    "                      stride=(1,2,1),\n",
    "                      padding=(0,0,0),\n",
    "                      ),\n",
    "            nn.LeakyReLU()\n",
    "            )\n",
    "        nn.init.kaiming_normal_(self.conv7[0].weight)\n",
    "\n",
    "        self.conv8 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=256,\n",
    "                      out_channels=256,\n",
    "                      kernel_size=(1,3,1),\n",
    "                      stride=(1,2,1),\n",
    "                      padding=(0,0,0),\n",
    "                      ),\n",
    "            nn.LeakyReLU()\n",
    "            )\n",
    "        nn.init.kaiming_normal_(self.conv8[0].weight)\n",
    "\n",
    "        self.linear1 = nn.Sequential(\n",
    "              nn.Linear(in_features=2560,\n",
    "                        out_features=1024,\n",
    "                        ),\n",
    "              nn.LeakyReLU(),\n",
    "              )\n",
    "        nn.init.kaiming_normal_(self.linear1[0].weight)\n",
    "\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(in_features=1024,\n",
    "                      out_features=1,\n",
    "                      ),\n",
    "            )\n",
    "        nn.init.kaiming_normal_(self.linear2[0].weight)\n",
    "        \n",
    "\n",
    "    def forward(self,song):\n",
    "        # Dimension of song is (batch, 5, N_bars, 96, 128)\n",
    "        out = self.conv1(song)\n",
    "        # Dimension of output is (batch, 128, N_bars-1, 96, 128)\n",
    "        out = self.conv2(out)\n",
    "        # Dimension of output is (batch, 128, 1, 96, 128)\n",
    "        out = self.conv3(out)\n",
    "        # Dimension of output is (batch, 128, 1, 96, 11)\n",
    "        out = self.conv4(out)\n",
    "        # Dimension of output is (batch, 128, 1, 96, 2)\n",
    "        out = self.conv5(out)\n",
    "        # Dimension of output is (batch, 128, 1, 48, 2)\n",
    "        out = self.conv6(out)\n",
    "        # Dimension of output is (batch, 128, 1, 24, 2)\n",
    "        out = self.conv7(out)\n",
    "        # Dimension of output is (batch, 256, 1, 11, 2)\n",
    "        out = self.conv8(out)\n",
    "        # Dimension of output is (batch, 256, 1, 5, 2)\n",
    "        out = nn.Flatten()(out)\n",
    "        # Dimension of output is (batch, 2560)\n",
    "        out = self.linear1(out)\n",
    "        # Dimension of output is (batch, 1024)\n",
    "        out = self.linear2(out)\n",
    "        # Dimension of output is (batch, 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmonyGAN:\n",
    "    \"\"\"\n",
    "    A GAN Model for music accompaniement, based on MuseGAN. It uses the Generator and Discriminator classes\n",
    "    defined above\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self,\n",
    "                 grad_weight = 10,\n",
    "                 z_dim = 32,\n",
    "                 batch_size = 16,\n",
    "                 n_bars = 3,\n",
    "                 n_steps_per_bar = 96,\n",
    "                 melody_embed_dim = 16,\n",
    "                 bar_embed_dim = 16,\n",
    "                ):\n",
    "    \n",
    "        self.name = 'HarmonyGAN'\n",
    "    \n",
    "        self.z_dim = z_dim\n",
    "    \n",
    "        self.n_tracks = 5\n",
    "        self.n_bars = n_bars\n",
    "        self.n_steps_per_bar = n_steps_per_bar\n",
    "        self.n_pitches = 128\n",
    "        \n",
    "        self.input_dim = (self.n_tracks,n_bars,n_steps_per_bar,self.n_pitches)\n",
    "\n",
    "        self.grad_weight = grad_weight\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Keep losses in memory during training:\n",
    "        self.d_losses = []\n",
    "        self.g_losses = []\n",
    "        \n",
    "        # Initialize number of epochs trained:\n",
    "        self.epoch = 0\n",
    "        \n",
    "        # Build Model:\n",
    "        self.D = Discriminator(self.input_dim).float()\n",
    "        self.G = Generator(self.batch_size,\n",
    "                           self.input_dim, \n",
    "                           self.z_dim, \n",
    "                           melody_embed_dim,\n",
    "                           bar_embed_dim).float()\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def _reset_gradient(self):\n",
    "        self.G_optimizer.zero_grad()\n",
    "        self.D_optimizer.zero_grad()\n",
    "        \n",
    "    \n",
    "    def train(self,dataset,epochs=500,save_every_n_epochs=50,d_loops=1,clamp_weights=0.01,lr_G = 0.00005,lr_D = 0.00005):\n",
    "        \"\"\"\n",
    "        Train the model on the given dataset using Adam optimizer and WGAN algorithm on three G losses\n",
    "        \"\"\"\n",
    "\n",
    "        # Build Optimizers:\n",
    "        self.G_optimizer = torch.optim.RMSprop(self.G.parameters(), lr=lr_G)\n",
    "        self.D_optimizer = torch.optim.RMSprop(self.D.parameters(), lr=lr_D)\n",
    "\n",
    "        # Iterator for epochs. Epochs is added to the number of already-trained epochs.\n",
    "        tqdm_epochs = tqdm(range(self.epoch,self.epoch+epochs), desc='Training ', unit='epoch',initial=self.epoch,total=self.epoch+epochs)\n",
    "\n",
    "\n",
    "\n",
    "        for epoch in tqdm_epochs:\n",
    "            \n",
    "            self.epoch+=1\n",
    "\n",
    "            # Randomly shuffle batches:\n",
    "            np.random.shuffle(dataset.items)\n",
    "    \n",
    "            tqdm_dataloader = tqdm(range(len(dataset)), desc='D_loss  ... - G_loss  ...', unit='batches', leave = False)\n",
    "            losses_d_internal = []\n",
    "            losses_g_internal = []\n",
    "    \n",
    "            for i in tqdm_dataloader:\n",
    "                true_songs = dataset[i]\n",
    "\n",
    "                \n",
    "\n",
    "                # Random permutations within the batch:\n",
    "                idx = torch.randperm(true_songs.shape[0])\n",
    "                true_songs = true_songs[idx]\n",
    "                true_songs = true_songs[:self.batch_size]\n",
    "\n",
    "                batch_size = true_songs.shape[0]\n",
    "\n",
    "\n",
    "            \n",
    "                # true_song is of shape (batch,n_bars,n_steps_per_bar,n_pitches,n_tracks) and uses int8\n",
    "                true_songs = true_songs.permute(0,4,1,2,3).float()\n",
    "                # Change the range of the notes to [-1,1]:\n",
    "                true_songs = 2*(true_songs-0.5)\n",
    "            \n",
    "                # true_song is of shape (batch,n_tracks,n_bars,n_steps_per_bar,n_pitches)\n",
    "                true_melodies = true_songs[:,0,:,:,:].unsqueeze(1)\n",
    "                # true_melody is of shape (batch,1,n_bars,n_steps_per_bar,n_pitches)\n",
    "\n",
    "                ####################################################################\n",
    "                #################### Train Discriminator ###########################\n",
    "                ####################################################################\n",
    "                for loop in range(d_loops):\n",
    "                    # D loops are used to insure that the D losses are not nullified by the slower learning of the generator\n",
    "                    # This is due to the fact that G has significantly more parameters than D, as is common in GANs\n",
    "\n",
    "\n",
    "                    # Generate fake song batch:\n",
    "                    chord_noise = torch.randn(batch_size,1,self.z_dim,1)\n",
    "                    style_noise = torch.randn(batch_size,1,self.z_dim,1)\n",
    "                    tracks_noise = torch.randn(batch_size,self.n_tracks-1,self.z_dim,1)\n",
    "                    groove_noise = torch.randn(batch_size,self.n_tracks-1,1,self.z_dim,1)\n",
    "                    fake_songs = self.G(chord_noise,style_noise,tracks_noise,groove_noise,true_melodies) #Use real melodies to generate the output\n",
    "\n",
    "                    # Tensors containing labels of either true or fake songs and neutral labels for the random average of images:\n",
    "                    positive_labels = torch.ones(batch_size,1)\n",
    "                    negative_labels = -torch.ones(batch_size,1)\n",
    "                    neutral_labels = torch.zeros(batch_size,1)\n",
    "\n",
    "                    ################## For True Songs ############### \n",
    "                    # Forward pass:\n",
    "                    true_scores = self.D(true_songs)\n",
    "                    # Compute discriminator Loss:\n",
    "                    d_loss_real = torch.mean(true_scores)\n",
    "                    \n",
    "\n",
    "                    ################# For Fake Songs ################\n",
    "                    # Forward pass:\n",
    "                    fake_scores = self.D(fake_songs)\n",
    "                    # Compute discriminator Loss:\n",
    "                    d_loss_fake = torch.mean(fake_scores)\n",
    "\n",
    "\n",
    "                    ################### Total Loss ##################\n",
    "                    d_loss = -d_loss_real + d_loss_fake\n",
    "                    # Reset gradients of both G and D optimizers:\n",
    "                    self._reset_gradient()\n",
    "                    # Backpropagation on D:\n",
    "                    d_loss.backward()\n",
    "                    # Optimize the weights of D:\n",
    "                    self.D_optimizer.step()\n",
    "\n",
    "\n",
    "                    # Add loss to list:\n",
    "                    losses_d_internal.append(d_loss.data.item())\n",
    "\n",
    "\n",
    "                    # Clamp weights of D:\n",
    "                    for param in self.D.parameters():\n",
    "                        param.data.clamp_(-clamp_weights, clamp_weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                ####################################################################\n",
    "                ######################## Train Generator ###########################\n",
    "                ####################################################################\n",
    "\n",
    "                # Generate fake song batch:\n",
    "                chord_noise = torch.randn(batch_size,1,self.z_dim,1)\n",
    "                style_noise = torch.randn(batch_size,1,self.z_dim,1)\n",
    "                tracks_noise = torch.randn(batch_size,self.n_tracks-1,self.z_dim,1)\n",
    "                groove_noise = torch.randn(batch_size,self.n_tracks-1,1,self.z_dim,1)\n",
    "                fake_songs = self.G(chord_noise,style_noise,tracks_noise,groove_noise,true_melodies) #Use real melodies to generate the output\n",
    "\n",
    "\n",
    "                # Get discriminator's predictions:\n",
    "                outputs = self.D(fake_songs)\n",
    "\n",
    "                # Get generator Loss to have D predict the output as a real image:\n",
    "                g_loss = -torch.mean(outputs)\n",
    "                losses_g_internal.append(g_loss.data.item())\n",
    "\n",
    "                # Reset gradient of both G and D:\n",
    "                self._reset_gradient()\n",
    "\n",
    "                # Backpropagation on both G and D:\n",
    "                g_loss.backward()\n",
    "\n",
    "                # Optimize only the weights of G:\n",
    "                self.G_optimizer.step()\n",
    "\n",
    "                # Show losses during training:\n",
    "                tqdm_dataloader.set_description(desc='D_loss  '+str(round(np.mean(losses_d_internal),3))+' - G_loss  '+str(round(np.mean(losses_g_internal),3)), refresh=True)\n",
    "\n",
    "            # Average losses across epoch:\n",
    "            self.d_losses.append(np.mean(losses_d_internal))\n",
    "            self.g_losses.append(np.mean(losses_g_internal))\n",
    "\n",
    "            # Save every n epochs:\n",
    "            if self.epoch%save_every_n_epochs==0:\n",
    "                print('Model and losses saved.')\n",
    "                self.save_model(version='intermediary')\n",
    "    \n",
    "\n",
    "    def _binarize(self,generated,thresh):\n",
    "        \"\"\"\n",
    "        Takes an array of probabilities as an input and returns the binarized version\n",
    "        \"\"\"\n",
    "        return np.where(generated>thresh, 1.0, 0.0)\n",
    "\n",
    "    \n",
    "    def accompaniement(self,conditionnal_track,thresh=0.2):\n",
    "        \"\"\" \n",
    "        Creates an accompaniement for the conditionnal track with random noises\n",
    "        Returns an array, binarized.\n",
    "        \n",
    "        \"\"\"\n",
    "        chord_noise = torch.randn(1,1,self.z_dim,1)\n",
    "        style_noise = torch.randn(1,1,self.z_dim,1)\n",
    "        tracks_noise = torch.randn(1,self.n_tracks-1,self.z_dim,1)\n",
    "        groove_noise = torch.randn(1,self.n_tracks-1,1,self.z_dim,1)\n",
    "        generated = self.G(chord_noise,style_noise,tracks_noise,groove_noise,conditionnal_track).data.numpy()\n",
    "        return self._binarize(generated,thresh)\n",
    "    \n",
    "    \n",
    "    def show_losses(self,directory='',show=True):\n",
    "        \"\"\"\n",
    "        Displays all losses and saves them in the running directory\n",
    "        \"\"\"\n",
    "        \n",
    "        fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "        #plt.plot([x[0] for x in self.d_losses], label='Critic loss on real scores', alpha=0.7)\n",
    "        plt.plot(self.d_losses, label='Critic loss', alpha=1)\n",
    "        #plt.plot([x[1] for x in self.d_losses], label='Critic loss on generated scores', alpha=0.7)\n",
    "        #plt.plot([x[2] for x in self.d_losses], label='Critic Partial loss (avg scores)', alpha=0.7)\n",
    "        plt.plot(self.g_losses, label='Generator loss', alpha=1)\n",
    "        \n",
    "        \n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "        plt.xlabel('Epochs', fontsize=18)\n",
    "        plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "        plt.savefig(directory+'losses.png')\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "    \n",
    "\n",
    "    def save_model(self,version='final'):\n",
    "        \"\"\" \n",
    "        Method to save the model either at the end of the run or at a certain epoch\n",
    "        \"\"\"\n",
    "        \n",
    "        if not os.path.isdir('Models'):\n",
    "            os.mkdir('Models')\n",
    "            os.mkdir('Models/final/')\n",
    "            os.mkdir('Models/intermediary/')\n",
    "        \n",
    "        \n",
    "        if version=='final':\n",
    "            directory = 'Models/final/'\n",
    "            subdirs = glob.glob(directory+'*/')\n",
    "            last_run_number = -1\n",
    "            for d in subdirs:\n",
    "                run_number = int(d.split('_')[-1][:-1])\n",
    "                if run_number>last_run_number:\n",
    "                    last_run_number=run_number\n",
    "            directory=directory+'Model_'+str(last_run_number+1)\n",
    "            os.mkdir(directory)\n",
    "            \n",
    "        elif version=='intermediary':\n",
    "            directory = 'Models/intermediary/'\n",
    "            subdirs = glob.glob(directory+'*/')\n",
    "    \n",
    "            \n",
    "            directory=directory+'Epoch_'+str(self.epoch)\n",
    "            os.mkdir(directory)\n",
    "        \n",
    "        # Save models:\n",
    "        torch.save(self.G,directory+'/Generator.ckpt')\n",
    "        torch.save(self.D,directory+'/Discriminator.ckpt')\n",
    "        torch.save(self.G.bar_encoder,directory+'/BarEncoder.ckpt')\n",
    "        torch.save(self.G.melody_encoder,directory+'/MelodyEncoder.ckpt')\n",
    "        torch.save(self.G.chord_temporal_network,directory+'/ChordsTemporalNetwork.ckpt')\n",
    "        for i in range(self.n_tracks-1):\n",
    "            torch.save(self.G.track_temporal_networks[i],directory+'/TracksTemporalNetwork_'+str(i+1)+'.ckpt')\n",
    "            torch.save(self.G.track_bar_generators[i],directory+'/BarGenerator_'+str(i+1)+'.ckpt')\n",
    "            \n",
    "        # Save losses:\n",
    "        self.show_losses(directory=directory+'/',show=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmony = HarmonyGAN(n_bars=8,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmony.show_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_np=np.load('/content/gdrive/My Drive/Perso/H2/small_dataset.npy').astype(np.float32)\n",
    "# Order: Drums, Piano, Guitar, Bass, Strings\n",
    "final_np = final_np[:,:,:,:,[1,2,4,3,0]]\n",
    "# Order: Piano, Guitar, Strings, Bass, Drums\n",
    "ds = TensorDataset(torch.from_numpy(final_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[reference_song] = ds[15]\n",
    "reference_song = reference_song.unsqueeze(0).permute(0,4,1,2,3)\n",
    "melody = reference_song[:,0,:,:,:].unsqueeze(1)\n",
    "print(melody.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accompaniement = harmony.accompaniement(melody,thresh=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accompaniement.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_song_to_array(t_song):\n",
    "    if type(t_song)==torch.Tensor:\n",
    "        t_song = t_song.data.numpy()\n",
    "    _,nb_tracks,nb_bars,steps_per_bar,pitches = t_song.shape\n",
    "    song = t_song.reshape((nb_tracks,nb_bars*steps_per_bar,pitches))\n",
    "    return song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accompaniement = tensor_song_to_array(accompaniement)\n",
    "reference_song = tensor_song_to_array(reference_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])  \n",
    "install('pypianoroll')\n",
    "import pypianoroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_pypianoroll(array,tempo=60):\n",
    "    # Order: Piano, Guitar, Strings, Bass, Drums\n",
    "    programs = [1, # Accoustic Piano\n",
    "                29, # Electric muted guitar\n",
    "                49, # Orchestral Strings\n",
    "                34, # Electric Bass Finger\n",
    "                118, # DrumSet\n",
    "               ]\n",
    "    is_drum = [False,False,False,False,True]\n",
    "    tracks = []\n",
    "    for track in range(array.shape[0]):\n",
    "        tracks.append(pypianoroll.Track(pianoroll=array[track,:,:],\n",
    "                                        program=programs[track], \n",
    "                                        is_drum=is_drum[track]))\n",
    "    return pypianoroll.Multitrack(tracks=tracks,tempo=tempo,beat_resolution=96//4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accompaniement = array_to_pypianoroll(accompaniement)\n",
    "reference_song = array_to_pypianoroll(reference_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=pypianoroll.plot_multitrack(reference_song,track_label='program')\n",
    "fig.set_size_inches(10,10)\n",
    "plt.savefig('pianoroll_reference.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=pypianoroll.plot_multitrack(accompaniement,track_label='program')\n",
    "fig.set_size_inches(10,10)\n",
    "plt.savefig('pianoroll_generated.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_song.write('reference.mid')\n",
    "accompaniement.write('generated.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
